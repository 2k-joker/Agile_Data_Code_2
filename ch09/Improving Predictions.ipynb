{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Predictions\n",
    "\n",
    "Now that we have deployed working models predicting flight delays, it is time to “make believe” that our prediction has proven useful based on user feedback, and further that the prediction is valuable enough that prediction quality is important. In this case, it is time to iteratively improve the quality of our prediction. If a prediction is valuable enough, this becomes a full-time job for one or more people.\n",
    "\n",
    "In this chapter we will tune our Spark ML classifier and also do additional feature engineering to improve prediction quality. In doing so, we will show you how to iteratively improve predictions.\n",
    "\n",
    "## Fixing Our Prediction Problem\n",
    "\n",
    "At this point we realized that our model was always predicting one class, no matter the input. We began by investigating that in a Jupyter notebook at [ch09/Debugging Prediction Problems.ipynb](Debugging Prediction Problems.ipynb).\n",
    "\n",
    "The notebook itself is very long, and we tried many things to fix our model. It turned out we had made a mistake. We were using `OneHotEncoder` on top of the output of `StringIndexerModel` when we were encoding our nominal/categorical string features. This is how you should encode features for models other than decision trees, but it turns out that for decision tree models, you are supposed to take the string indexes from `StringIndexerModel` and directly compose them with your continuous/numeric features in a `VectorAssembler`. Decision trees are able to infer the fact that indexes represent categories. One benefit of directly adding StringIndexes to your feature vectors is that you then get easily interpretable feature importances.\n",
    "\n",
    "When we discovered this, we had to go back and edit the book so that we didn’t teach something that was wrong, and so this is now what you see. We thought it worthwhile to link to the notebook, though, to show how this really works in the wild: you build broken shit and then fix it.\n",
    "\n",
    "## When to Improve Predictions\n",
    "\n",
    "Not all predictions should be improved. Often something fast and crude will work well enough as an MVP (minimum viable product). Only predictions that prove useful should be improved. It is possible to sink large volumes of time into improving the quality of a prediction, so it is essential that you connect with users before getting sucked into this task. This is why we’ve included the discussion of improving predictions in its own chapter.\n",
    "\n",
    "## Improving Prediction Performance\n",
    "\n",
    "There are a few ways to improve an existing predictive model. The first is by tuning the parameters of the statistical model making your prediction. The second is feature engineering.\n",
    "\n",
    "Tuning model hyperparameters to improve predictive model quality can be done by intuition, or by brute force through something called a grid or random search. We’re going to focus on feature engineering, as hyperparameter tuning is covered elsewhere. A good guide to hyperparameter tuning is available in the Spark documentation on model selection and tuning.\n",
    "\n",
    "As we move through this chapter, we’ll be using the work we’ve done so far to perform feature engineering. Feature engineering is the most important part of making good predictions. It involves using what you’ve discovered about the data through exploratory data analysis in order to feed your machine learning algorithm better, more consequential data as input.\n",
    "\n",
    "### Experimental Adhesion Method: See What Sticks\n",
    "\n",
    "There are several ways to decide which features to use, and Saurav Kaushik has written a post on Analytics Vidhya that introduces them well. The method we employ primarily, which we jokingly entitle the Experimental Adhesion Method, is to quickly select all the features that we can simply compute, and try them all using a random forest or gradient boosted decision tree model (note that even if our application requires another type of model, we still use decision trees to guide feature selection). Then we train the model and inspect the model’s feature importances to “see what sticks.” The most important variables are retained, and this forms the basic model we begin with.\n",
    "\n",
    "Feature engineering is an iterative process. Based on the feature importances, we ponder what new things we might try using the data we have available. We start with the simplest idea, or the one that is easiest to implement. If the feature importances indicate one type of feature is important, and we can’t easily compute new features similar to this one, we think about how we might acquire new data to join to our training data to use as features.\n",
    "\n",
    "The key is to be logical and systematic in our exploration of the feature space. You should think about how easy a potential feature is to compute, as well as what it would teach you if it turned out to be important. Are there other, similar features that you could try if this candidate worked? Develop hypotheses and test them in the form of new features. Evaluate each new feature in an experiment and reflect on what you’ve learned before engineering the next feature.\n",
    "\n",
    "### Establishing Rigorous Metrics for Experiments\n",
    "\n",
    "In order to improve our classification model, we need to reliably determine its prediction quality in the first place. To do so, we need to beef up our cross-validation code, and then establish a baseline of quality for the original model. Check out [ch09/baseline_spark_mllib_model.py](baseline_spark_mllib_model.py), which we copied from [ch09/train_spark_mllib_model.py](train_spark_mllib_model.py) and altered to improve its cross-validation code.\n",
    "\n",
    "In order to evaluate the prediction quality of our classifier, we need to use more than one metric. Spark ML’s `MulticlassClassificationEvaluator` offers four metrics: accuracy, weighted precision, weighted recall, and f1.\n",
    "\n",
    "### Defining Our Classification Metrics\n",
    "\n",
    "The raw _accuracy_ is just what it sounds like: the number of correct predictions divided by the number of predictions. This is something to check first, but it isn’t adequate alone. _Precision_ is a measure of how useful the result is. _Recall_ describes how complete the results are. The _f1_ score incorporates both precision and recall to determine overall quality. Taken together, the changes to these metrics between consecutive runs of training our model can give us a clear picture of what is happening with our model in terms of prediction quality. We will use these metrics along with feature importance to guide our feature engineering efforts.\n",
    "\n",
    "### Feature Importance\n",
    "\n",
    "Model quality metrics aren’t enough to guide the iterative improvements of our model. To understand what is going on with each new run, we need to employ a type of model called a decision tree.\n",
    "\n",
    "In Spark ML, the best general-purpose multiclass classification model is an implementation of a random forest, the RandomForestClassificationModel, fit by the RandomForestClassifier. Random forests can classify or regress, and they have an important feature that helps us interrogate predictive models through a feature called feature importance.\n",
    "\n",
    "The importance of a feature is what it sounds like: a measure of how important that feature was in contributing to the accuracy of the model. This information is incredibly useful, as it can serve as a guiding hand to feature engineering. In other words, if you know how important a feature is, you can use this clue to make changes that increase the accuracy of the model, such as removing unimportant features and trying to engineer features similar to those that are most important. Feature engineering is a major theme of Agile Data Science, and it is a big part of why we’ve been doing iterative visualization and exploration (the purpose of which is to shed light on and drive feature engineering).\n",
    "\n",
    "Note that the state of the art for many classification and regression tasks is a gradient boosted decision tree, but as of version 2.1.0 Spark ML’s implementation—the `GBTClassificationModel`, which is fit by the `GBTClassifier`—can only do binary classification.\n",
    "\n",
    "### Getting Ready for Experiments\n",
    "\n",
    "We need to run through the model's code from chapter 8 before we can setup and run an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark initialized...\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re\n",
    "import json\n",
    "import datetime, iso8601\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Initialize PySpark\n",
    "APP_NAME = \"Improving Predictions\"\n",
    "\n",
    "# If there is no SparkSession, create the environment\n",
    "try:\n",
    "    sc and spark\n",
    "except NameError as e:\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    import pyspark\n",
    "    import pyspark.sql\n",
    "\n",
    "    sc = pyspark.SparkContext()\n",
    "    spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "print(\"PySpark initialized...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ArrDelay=143.0, CRSArrTime=datetime.datetime(2015, 1, 18, 22, 4), CRSDepTime=datetime.datetime(2015, 1, 18, 20, 55), Carrier='B6', DayOfMonth=18, DayOfWeek=7, DayOfYear=18, DepDelay=147.0, Dest='BOS', Distance=187.0, FlightDate=datetime.date(2015, 1, 18), FlightNum='418', Origin='JFK')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, DateType, TimestampType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ArrDelay\", DoubleType(), True),     # \"ArrDelay\":5.0\n",
    "    StructField(\"CRSArrTime\", TimestampType(), True),    # \"CRSArrTime\":\"2015-12-31T03:20:00.000-08:00\"\n",
    "    StructField(\"CRSDepTime\", TimestampType(), True),    # \"CRSDepTime\":\"2015-12-31T03:05:00.000-08:00\"\n",
    "    StructField(\"Carrier\", StringType(), True),     # \"Carrier\":\"WN\"\n",
    "    StructField(\"DayOfMonth\", IntegerType(), True), # \"DayOfMonth\":31\n",
    "    StructField(\"DayOfWeek\", IntegerType(), True),  # \"DayOfWeek\":4\n",
    "    StructField(\"DayOfYear\", IntegerType(), True),  # \"DayOfYear\":365\n",
    "    StructField(\"DepDelay\", DoubleType(), True),     # \"DepDelay\":14.0\n",
    "    StructField(\"Dest\", StringType(), True),        # \"Dest\":\"SAN\"\n",
    "    StructField(\"Distance\", DoubleType(), True),     # \"Distance\":368.0\n",
    "    StructField(\"FlightDate\", DateType(), True),    # \"FlightDate\":\"2015-12-30T16:00:00.000-08:00\"\n",
    "    StructField(\"FlightNum\", StringType(), True),   # \"FlightNum\":\"6109\"\n",
    "    StructField(\"Origin\", StringType(), True),      # \"Origin\":\"TUS\"\n",
    "])\n",
    "\n",
    "input_path = \"../data/simple_flight_delay_features.json\"\n",
    "features = spark.read.json(input_path, schema=schema)\n",
    "\n",
    "# Sample 10% to make executable inside the notebook\n",
    "features = features.sample(False, 0.1)\n",
    "\n",
    "features.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with nulls that need to be filtered: []\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Check for nulls in features before using Spark ML\n",
    "#\n",
    "null_counts = [(column, features.where(features[column].isNull()).count()) for column in features.columns]\n",
    "cols_with_nulls = filter(lambda x: x[1] > 0, null_counts)\n",
    "print(\"Columns with nulls that need to be filtered: {}\".format(\n",
    "    str(list(cols_with_nulls))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+\n",
      "|ArrDelay|         CRSArrTime|         CRSDepTime|Carrier|DayOfMonth|DayOfWeek|DayOfYear|DepDelay|Dest|Distance|FlightDate|FlightNum|Origin|  Route|\n",
      "+--------+-------------------+-------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+\n",
      "|   143.0|2015-01-18 22:04:00|2015-01-18 20:55:00|     B6|        18|        7|       18|   147.0| BOS|   187.0|2015-01-18|      418|   JFK|JFK-BOS|\n",
      "|    -5.0|2015-01-19 00:04:00|2015-01-18 23:00:00|     B6|        18|        7|       18|     0.0| BOS|   187.0|2015-01-18|      718|   JFK|JFK-BOS|\n",
      "|   -17.0|2015-01-18 10:35:00|2015-01-18 07:22:00|     B6|        18|        7|       18|    -3.0| FLL|  1069.0|2015-01-18|      901|   JFK|JFK-FLL|\n",
      "+--------+-------------------+-------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Add a Route variable to replace FlightNum\n",
    "#\n",
    "from pyspark.sql.functions import lit, concat\n",
    "features_with_route = features.withColumn(\n",
    "  'Route',\n",
    "  concat(\n",
    "    features.Origin,\n",
    "    lit('-'),\n",
    "    features.Dest\n",
    "  )\n",
    ")\n",
    "features_with_route.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|ArrDelay|ArrDelayBucket|\n",
      "+--------+--------------+\n",
      "|   143.0|           3.0|\n",
      "|    -5.0|           1.0|\n",
      "|   -17.0|           0.0|\n",
      "|     3.0|           2.0|\n",
      "|    -7.0|           1.0|\n",
      "+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Use pysmark.ml.feature.Bucketizer to bucketize ArrDelay into on-time, slightly late, very late (0, 1, 2)\n",
    "#\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Setup the Bucketizer\n",
    "splits = [-float(\"inf\"), -15.0, 0, 30.0, float(\"inf\")]\n",
    "arrival_bucketizer = Bucketizer(\n",
    "  splits=splits,\n",
    "  inputCol=\"ArrDelay\",\n",
    "  outputCol=\"ArrDelayBucket\"\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "arrival_bucketizer_path = \"../models/arrival_bucketizer_2.0.bin\"\n",
    "arrival_bucketizer.write().overwrite().save(arrival_bucketizer_path)\n",
    "\n",
    "# Apply the model\n",
    "ml_bucketized_features = arrival_bucketizer.transform(features_with_route)\n",
    "ml_bucketized_features.select(\"ArrDelay\", \"ArrDelayBucket\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing column \"Carrier\" ...\n",
      "Indexing column \"DayOfMonth\" ...\n",
      "Indexing column \"DayOfWeek\" ...\n",
      "Indexing column \"DayOfYear\" ...\n",
      "Indexing column \"Origin\" ...\n",
      "Indexing column \"Dest\" ...\n",
      "Indexing column \"Route\" ...\n",
      "Indexed all string columns!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Extract features tools in with pyspark.ml.feature\n",
    "#\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "# Turn category fields into indexes\n",
    "for column in [\"Carrier\", \"DayOfMonth\", \"DayOfWeek\", \"DayOfYear\",\n",
    "             \"Origin\", \"Dest\", \"Route\"]:\n",
    "    \n",
    "    print(\"Indexing column \\\"{}\\\" ...\".format(column))\n",
    "    \n",
    "    string_indexer = StringIndexer(\n",
    "      inputCol=column,\n",
    "      outputCol=column + \"_index\"\n",
    "    )\n",
    "\n",
    "    string_indexer_model = string_indexer.fit(ml_bucketized_features)\n",
    "    ml_bucketized_features = string_indexer_model.transform(ml_bucketized_features)\n",
    "\n",
    "    # Drop the original column\n",
    "    ml_bucketized_features = ml_bucketized_features.drop(column)\n",
    "\n",
    "    # Save the pipeline model\n",
    "    string_indexer_output_path = \"../models/string_indexer_model_{}.bin\".format(\n",
    "      column\n",
    "    )\n",
    "    string_indexer_model.write().overwrite().save(string_indexer_output_path)\n",
    "\n",
    "print(\"Indexed all string columns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+--------+--------+----------+---------+--------------+--------------------+\n",
      "|ArrDelay|         CRSArrTime|         CRSDepTime|DepDelay|Distance|FlightDate|FlightNum|ArrDelayBucket|        Features_vec|\n",
      "+--------+-------------------+-------------------+--------+--------+----------+---------+--------------+--------------------+\n",
      "|   143.0|2015-01-18 22:04:00|2015-01-18 20:55:00|   147.0|   187.0|2015-01-18|      418|           3.0|[147.0,187.0,8.0,...|\n",
      "|    -5.0|2015-01-19 00:04:00|2015-01-18 23:00:00|     0.0|   187.0|2015-01-18|      718|           1.0|[0.0,187.0,8.0,25...|\n",
      "|   -17.0|2015-01-18 10:35:00|2015-01-18 07:22:00|    -3.0|  1069.0|2015-01-18|      901|           0.0|[-3.0,1069.0,8.0,...|\n",
      "|     3.0|2015-01-18 12:48:00|2015-01-18 09:57:00|     5.0|  2248.0|2015-01-18|      411|           2.0|[5.0,2248.0,8.0,2...|\n",
      "|    -7.0|2015-01-18 10:50:00|2015-01-18 07:50:00|    -4.0|   944.0|2015-01-18|     1783|           1.0|[-4.0,944.0,8.0,2...|\n",
      "+--------+-------------------+-------------------+--------+--------+----------+---------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handle continuous, numeric fields by combining them into one feature vector\n",
    "numeric_columns = [\"DepDelay\", \"Distance\"]\n",
    "index_columns = [\"Carrier_index\", \"DayOfMonth_index\",\n",
    "                 \"DayOfWeek_index\", \"DayOfYear_index\", \"Origin_index\",\n",
    "                 \"Origin_index\", \"Dest_index\", \"Route_index\"]\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=numeric_columns + index_columns,\n",
    "    outputCol=\"Features_vec\"\n",
    ")\n",
    "final_vectorized_features = vector_assembler.transform(ml_bucketized_features)\n",
    "\n",
    "# Save the numeric vector assembler\n",
    "vector_assembler_path = \"../models/numeric_vector_assembler.bin\"\n",
    "vector_assembler.write().overwrite().save(vector_assembler_path)\n",
    "\n",
    "# Drop the index columns\n",
    "for column in index_columns:\n",
    "    final_vectorized_features = final_vectorized_features.drop(column)\n",
    "\n",
    "# Inspect the finalized features\n",
    "final_vectorized_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing A More Rigorous Experiment\n",
    "\n",
    "In order to be confident in our experiment for each measure, we need to repeat it at least twice to see how it varies. This is the degree to which we cross-validate. In addition, we need to loop and run the measurement code once for each score. Once we’ve collected several scores for each metric, we look at both the average and standard deviation for each score. Taken together, these scores give us a picture of the quality of our classifier.\n",
    "\n",
    "To begin, we need to iterate and repeat our experiment N times. For each experiment we need to compute a test/train split, then we need to train the model on the training data and apply it to the test data. Then we use `MulticlassClassificationEvaluator` to get a score, once for each metric. We gather the scores in a list for each metric, which we will evaluate at the end of the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 out of 3 of test/train splits in cross validation...\n",
      "accuracy = 0.5924365207995678\n",
      "weightedPrecision = 0.631295294700286\n",
      "weightedRecall = 0.5924365207995678\n",
      "f1 = 0.5414995624972146\n",
      "Run 2 out of 3 of test/train splits in cross validation...\n",
      "accuracy = 0.5896727111897391\n",
      "weightedPrecision = 0.6450428873803301\n",
      "weightedRecall = 0.5896727111897391\n",
      "f1 = 0.5342508851519681\n",
      "Run 3 out of 3 of test/train splits in cross validation...\n",
      "accuracy = 0.5956106040851804\n",
      "weightedPrecision = 0.6447813404593224\n",
      "weightedRecall = 0.5956106040851804\n",
      "f1 = 0.5444886343859805\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Cross validate, train and evaluate classifier: loop 5 times for 4 metrics\n",
    "#\n",
    "\n",
    "from collections import defaultdict\n",
    "scores = defaultdict(list)\n",
    "metric_names = [\"accuracy\", \"weightedPrecision\", \"weightedRecall\", \"f1\"]\n",
    "split_count = 3\n",
    "\n",
    "for i in range(1, split_count + 1):\n",
    "  print(\"Run {} out of {} of test/train splits in cross validation...\".format(\n",
    "      i,\n",
    "      split_count,\n",
    "    )\n",
    "  )\n",
    "\n",
    "  # Test/train split\n",
    "  training_data, test_data = final_vectorized_features.randomSplit([0.8, 0.2])\n",
    "\n",
    "  # Instantiate and fit random forest classifier on all the data\n",
    "  from pyspark.ml.classification import RandomForestClassifier\n",
    "  rfc = RandomForestClassifier(\n",
    "    featuresCol=\"Features_vec\",\n",
    "    labelCol=\"ArrDelayBucket\",\n",
    "    predictionCol=\"Prediction\",\n",
    "    maxBins=4657,\n",
    "    maxMemoryInMB=1024\n",
    "  )\n",
    "  model = rfc.fit(training_data)\n",
    "\n",
    "  # Save the new model over the old one\n",
    "  model_output_path = \"../models/spark_random_forest_classifier.flight_delays.baseline.bin\"\n",
    "  model.write().overwrite().save(model_output_path)\n",
    "\n",
    "  # Evaluate model using test data\n",
    "  predictions = model.transform(test_data)\n",
    "  \n",
    "  # Evaluate this split's results for each metric\n",
    "  from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "  for metric_name in metric_names:\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "      labelCol=\"ArrDelayBucket\",\n",
    "      predictionCol=\"Prediction\",\n",
    "      metricName=metric_name\n",
    "    )\n",
    "    score = evaluator.evaluate(predictions)\n",
    "\n",
    "    scores[metric_name].append(score)\n",
    "    print(\"{} = {}\".format(metric_name, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Run Results\n",
    "\n",
    "Our run leaves us with a `defaultdict` of scores, with one list for each metric. Now we need to compute the average and standard deviation of each list to give us the overall average and standard deviation of each metric:\n",
    "\n",
    "Note that we need to compute both the average and standard deviation of each metric from our run. The average will tell us the approximate performance level, and the standard deviation will tell us how much a model's performance varies. Less variance is desirable. We'll use this information in tuning our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment Log\n",
      "--------------\n",
      "Metric               Average         STD\n",
      "-----------------  ---------  ----------\n",
      "accuracy            0.592573  0.00242606\n",
      "weightedPrecision   0.640373  0.00641992\n",
      "weightedRecall      0.592573  0.00242606\n",
      "f1                  0.54008   0.00429844\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Evaluate average and STD of each metric and print a table\n",
    "#\n",
    "import numpy as np\n",
    "score_averages = defaultdict(float)\n",
    "\n",
    "# Compute the table data\n",
    "average_stds = [] # ha\n",
    "for metric_name in metric_names:\n",
    "  metric_scores = scores[metric_name]\n",
    "  \n",
    "  average_accuracy = sum(metric_scores) / len(metric_scores)\n",
    "  score_averages[metric_name] = average_accuracy\n",
    "  \n",
    "  std_accuracy = np.std(metric_scores)\n",
    "  \n",
    "  average_stds.append((metric_name, average_accuracy, std_accuracy))\n",
    "\n",
    "# Print the table\n",
    "print(\"\\nExperiment Log\")\n",
    "print(\"--------------\")\n",
    "print(tabulate(average_stds, headers=[\"Metric\", \"Average\", \"STD\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The standard deviations indicate that we might not even need to perform k-fold cross-validation, but an inspection of the underlying scores says otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'accuracy': [0.5924365207995678,\n",
       "              0.5896727111897391,\n",
       "              0.5956106040851804],\n",
       "             'weightedPrecision': [0.631295294700286,\n",
       "              0.6450428873803301,\n",
       "              0.6447813404593224],\n",
       "             'weightedRecall': [0.5924365207995678,\n",
       "              0.5896727111897391,\n",
       "              0.5956106040851804],\n",
       "             'f1': [0.5414995624972146,\n",
       "              0.5342508851519681,\n",
       "              0.5444886343859805]})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is actually significant variation between runs, and this could obscure a small improvement (or degradation) in prediction quality.\n",
    "\n",
    "The iterations take time, and this discourages experimentation. A middle ground should be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Experiments to Determine Improvements\n",
    "\n",
    "Now that we have our baseline metrics, we can repeat this code as we improve the model and see what the effect is in terms of the four metrics available to us. So it seems we are done, that we can start playing with our model and features. However, we will quickly run into a problem. We will lose track of the score from the previous run, printed on the screen above many logs for each run, unless we write it down each time. And this is tedious. So, we need to automate this process.\n",
    "\n",
    "What we need to do is load a score log from disk, evaluate the current score in terms of the previous one, and store a new entry to the log back to disk for the next run to access. The following code achieves this aim.\n",
    "\n",
    "First we use pickle to load any existing score log. If this is not present, we initialize a new log, which is simply an empty Python list. Next we prepare the new log entry—a simple Python dict containing the average score for each of four metrics. Then we subtract the previous run’s score to determine the change in this run. This is the information we use to evaluate whether our change worked or not (along with any changes in feature importances, which we will address as well). \n",
    "\n",
    "Finally, we append the new score entry to the log and store it back to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment Report\n",
      "-----------------\n",
      "Metric                   Score\n",
      "-----------------  -----------\n",
      "accuracy            0.00702217\n",
      "weightedPrecision  -0.0018761\n",
      "weightedRecall      0.00702217\n",
      "f1                  0.0105868\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Persist the score to a sccore log that exists between runs\n",
    "#\n",
    "import pickle\n",
    "\n",
    "# Load the score log or initialize an empty one\n",
    "try:\n",
    "  score_log_filename = \"../models/score_log.pickle\"\n",
    "  score_log = pickle.load(open(score_log_filename, \"rb\"))\n",
    "  if not isinstance(score_log, list):\n",
    "    score_log = []\n",
    "except IOError:\n",
    "  score_log = []\n",
    "\n",
    "# Compute the existing score log entry\n",
    "score_log_entry = {metric_name: score_averages[metric_name] for metric_name in metric_names}\n",
    "\n",
    "# Compute and display the change in score for each metric\n",
    "try:\n",
    "  last_log = score_log[-1]\n",
    "except (IndexError, TypeError, AttributeError):\n",
    "  last_log = score_log_entry\n",
    "\n",
    "experiment_report = []\n",
    "for metric_name in metric_names:\n",
    "  run_delta = score_log_entry[metric_name] - last_log[metric_name]\n",
    "  experiment_report.append((metric_name, run_delta))\n",
    "\n",
    "print(\"\\nExperiment Report\")\n",
    "print(\"-----------------\")\n",
    "print(tabulate(experiment_report, headers=[\"Metric\", \"Score\"]))\n",
    "\n",
    "# Append the existing average scores to the log\n",
    "score_log.append(score_log_entry)\n",
    "\n",
    "# Persist the log for next run\n",
    "pickle.dump(score_log, open(score_log_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we run our script, we will get a report that shows the change between this run and the last run. We can use this, along with our feature importances, to direct our efforts at improving the model. For instance, an example test run shows the model accuracy increase by .003:\n",
    "\n",
    "```\n",
    "Experiment Report\n",
    "-----------------\n",
    "Metric                   Score\n",
    "-----------------  -----------\n",
    "accuracy            0.00300548\n",
    "weightedPrecision  -0.00592227\n",
    "weightedRecall      0.00300548\n",
    "f1                 -0.0105553\n",
    "```\n",
    "\n",
    "Jump back to the code for the model, the code under the section `Implementing a More Rigorous Experiment`. Re-run all the code between there and here, the last three code blocks. See how the score changed slightly? You will use these changes to guide you as you change the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Changes in Feature Importance\n",
    "\n",
    "We can use the list of columns given to our final `VectorAssembler` along with `RandomForestClassificationModel.featureImportances` to derive the importance of each named feature. This is extremely valuable, because like with our prediction quality scores, we can look at changes in feature importances for all features between runs. If a newly introduced feature turns out to be important, it is usually worth adding to the model, so long as it doesn’t hurt quality.\n",
    "\n",
    "### Logging Feature Importances\n",
    "\n",
    "We begin by altering our experiment loop to record feature importances for each run. Check out the abbreviated content from [ch09/improved_spark_mllib_model.py](improved_spark_mllib_model.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1 out of 3 of test/train splits in cross validation...\n",
      "accuracy = 0.590385033717642\n",
      "weightedPrecision = 0.6372292720494213\n",
      "weightedRecall = 0.590385033717642\n",
      "f1 = 0.5403098159428674\n",
      "\n",
      "Run 2 out of 3 of test/train splits in cross validation...\n",
      "accuracy = 0.5981936336600947\n",
      "weightedPrecision = 0.6528951533575129\n",
      "weightedRecall = 0.5981936336600947\n",
      "f1 = 0.5469302844275894\n",
      "\n",
      "Run 3 out of 3 of test/train splits in cross validation...\n",
      "accuracy = 0.5913990201415351\n",
      "weightedPrecision = 0.6476422137879245\n",
      "weightedRecall = 0.5913990201415351\n",
      "f1 = 0.5398263868260617\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Cross validate, train and evaluate classifier: loop 5 times for 4 metrics\n",
    "#\n",
    "\n",
    "from collections import defaultdict\n",
    "scores = defaultdict(list)\n",
    "feature_importances = defaultdict(list)\n",
    "metric_names = [\"accuracy\", \"weightedPrecision\", \"weightedRecall\", \"f1\"]\n",
    "split_count = 3\n",
    "\n",
    "for i in range(1, split_count + 1):\n",
    "  print(\"\\nRun {} out of {} of test/train splits in cross validation...\".format(\n",
    "      i,\n",
    "      split_count,\n",
    "    )\n",
    "  )\n",
    "\n",
    "  # Test/train split\n",
    "  training_data, test_data = final_vectorized_features.randomSplit([0.8, 0.2])\n",
    "\n",
    "  # Instantiate and fit random forest classifier on all the data\n",
    "  from pyspark.ml.classification import RandomForestClassifier\n",
    "  rfc = RandomForestClassifier(\n",
    "    featuresCol=\"Features_vec\",\n",
    "    labelCol=\"ArrDelayBucket\",\n",
    "    predictionCol=\"Prediction\",\n",
    "    maxBins=4657,\n",
    "  )\n",
    "  model = rfc.fit(training_data)\n",
    "\n",
    "  # Save the new model over the old one\n",
    "  model_output_path = \"../models/spark_random_forest_classifier.flight_delays.baseline.bin\"\n",
    "  model.write().overwrite().save(model_output_path)\n",
    "\n",
    "  # Evaluate model using test data\n",
    "  predictions = model.transform(test_data)\n",
    "\n",
    "  # Evaluate this split's results for each metric\n",
    "  from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "  for metric_name in metric_names:\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "      labelCol=\"ArrDelayBucket\",\n",
    "      predictionCol=\"Prediction\",\n",
    "      metricName=metric_name\n",
    "    )\n",
    "    score = evaluator.evaluate(predictions)\n",
    "  \n",
    "    scores[metric_name].append(score)\n",
    "    print(\"{} = {}\".format(metric_name, score))\n",
    "\n",
    "  #\n",
    "  # Collect feature importances\n",
    "  #\n",
    "  feature_names = vector_assembler.getInputCols()\n",
    "  feature_importance_list = model.featureImportances\n",
    "  for feature_name, feature_importance in zip(feature_names, feature_importance_list):\n",
    "    feature_importances[feature_name].append(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Feature Importances\n",
    "\n",
    "Next, we need to compute the average of the importance for each feature. Note that we use a `defaultdict(float)` to ensure that accessing empty keys returns zero. This will be important when comparing entries in the log with different sets of features. In order to print the feature importances, we need to sort them first, by descending order of importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances\n",
      "-------------------\n",
      "Name                Importance\n",
      "----------------  ------------\n",
      "DepDelay            0.83023\n",
      "Route_index         0.050823\n",
      "DayOfYear_index     0.0209052\n",
      "Distance            0.0199261\n",
      "Origin_index        0.0198472\n",
      "Dest_index          0.0196378\n",
      "DayOfMonth_index    0.0115628\n",
      "Carrier_index       0.00580583\n",
      "DayOfWeek_index     0.0014148\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Analyze and report feature importance changes\n",
    "#\n",
    "\n",
    "# Compute averages for each feature\n",
    "feature_importance_entry = defaultdict(float)\n",
    "for feature_name, value_list in feature_importances.items():\n",
    "  average_importance = sum(value_list) / len(value_list)\n",
    "  feature_importance_entry[feature_name] = average_importance\n",
    "\n",
    "# Sort the feature importances in descending order and print\n",
    "import operator\n",
    "sorted_feature_importances = sorted(\n",
    "  feature_importance_entry.items(),\n",
    "  key=operator.itemgetter(1),\n",
    "  reverse=True\n",
    ")\n",
    "\n",
    "print(\"\\nFeature Importances\")\n",
    "print(\"-------------------\")\n",
    "print(tabulate(sorted_feature_importances, headers=['Name', 'Importance']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Differences Between Runs\n",
    "\n",
    "Next we need to perform the same housekeeping as we did for the model score log: load the model, create an entry for this experiment, load the last experiment and compute the change for each feature between that experiment and the current one, and then print a report on these deltas.\n",
    "\n",
    "First we load the last feature log. If it isn’t available because it doesn’t exist, we initialize the last_feature_log with zeros for each feature, so that new features will have a positive score equal to their amount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Compare this run's feature importances with the previous run's\n",
    "#\n",
    "  \n",
    "# Load the feature importance log or initialize an empty one\n",
    "try:\n",
    "  feature_log_filename = \"../models/feature_log.pickle\"\n",
    "  feature_log = pickle.load(open(feature_log_filename, \"rb\"))\n",
    "  if not isinstance(feature_log, list):\n",
    "    feature_log = []\n",
    "except IOError:\n",
    "  feature_log = []\n",
    "\n",
    "# Compute and display the change in score for each feature\n",
    "try:\n",
    "  last_feature_log = feature_log[-1]\n",
    "except (IndexError, TypeError, AttributeError):\n",
    "  last_feature_log = defaultdict(float)\n",
    "  for feature_name, importance in feature_importance_entry.items():\n",
    "    last_feature_log[feature_name] = importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute the change between the last run and the current one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the deltas\n",
    "feature_deltas = {}\n",
    "for feature_name in feature_importances.keys():\n",
    "  run_delta = feature_importance_entry[feature_name] - last_feature_log[feature_name]\n",
    "  feature_deltas[feature_name] = run_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to display them, we need to sort the feature importance changes in descending order, to show the biggest change first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort feature deltas, biggest change first\n",
    "import operator\n",
    "sorted_feature_deltas = sorted(\n",
    "  feature_deltas.items(),\n",
    "  key=operator.itemgetter(1),\n",
    "  reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we display the sorted feature deltas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance Delta Report\n",
      "-------------------------------\n",
      "Feature             Delta\n",
      "----------------  -------\n",
      "DepDelay                0\n",
      "Distance                0\n",
      "Carrier_index           0\n",
      "DayOfMonth_index        0\n",
      "DayOfWeek_index         0\n",
      "DayOfYear_index         0\n",
      "Origin_index            0\n",
      "Dest_index              0\n",
      "Route_index             0\n"
     ]
    }
   ],
   "source": [
    "# Display sorted feature deltas\n",
    "print(\"\\nFeature Importance Delta Report\")\n",
    "print(\"-------------------------------\")\n",
    "print(tabulate(sorted_feature_deltas, headers=[\"Feature\", \"Delta\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as with the score log, we append our entry to the log and save it for the next run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the existing average deltas to the log\n",
    "feature_log.append(feature_importance_entry)\n",
    "\n",
    "# Persist the log for next run\n",
    "pickle.dump(feature_log, open(feature_log_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use the raw feature importances as well as the changes in feature importance to guide our creation or alteration of features as we improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Now that we have the ability to understand the effect of changes between experimental runs, we can detect changes that improve our model. We can start adding features to test their effect on the model’s prediction quality, and pursue related features that help improve quality! Without this setup, we would be hard put to make positive changes. With it, we are only bounded by our creativity in our efforts to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time of Day as a Feature\n",
    "\n",
    "In examining our feature importances, it looks like the date/time fields have some impact. What if we extracted the hour/minute as an integer from the datetime for departure/arrival fields? This would inform the model about morning versus afternoon versus red-eye flights, which surely affects on-time performance, as there is more traffic in the morning than overnight.\n",
    "\n",
    "Check out [ch09/explore_delays.py](explore_delays.py). Let’s start by exploring the premise of this feature, that lateness varies by the time of day of the flight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
