{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize PySpark\n",
    "\n",
    "First, we use the findspark package to initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark initiated...\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark\n",
    "APP_NAME = \"Debugging Prediction Problems\"\n",
    "\n",
    "# If there is no SparkSession, create the environment\n",
    "try:\n",
    "  sc and spark\n",
    "except NameError as e:\n",
    "  import findspark\n",
    "  findspark.init()\n",
    "  import pyspark\n",
    "  import pyspark.sql\n",
    "\n",
    "  sc = pyspark.SparkContext()\n",
    "  spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "print(\"PySpark initiated...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, World!\n",
    "\n",
    "Loading data, mapping it and collecting the records into RAM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Russell Jurney', 'Relato', 'CEO'],\n",
       " ['Florian Liebert', 'Mesosphere', 'CEO'],\n",
       " ['Don Brown', 'Rocana', 'CIO'],\n",
       " ['Steve Jobs', 'Apple', 'CEO'],\n",
       " ['Donald Trump', 'The Trump Organization', 'CEO'],\n",
       " ['Russell Jurney', 'Data Syndrome', 'Principal Consultant']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text file using the SparkContext\n",
    "csv_lines = sc.textFile(\"../data/example.csv\")\n",
    "\n",
    "# Map the data to split the lines into a list\n",
    "data = csv_lines.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Collect the dataset into local RAM\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Objects from CSV using `pyspark.RDD.map`\n",
    "\n",
    "Using a function with a map operation to create objects (dicts) as records..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Russell Jurney', 'company': 'Relato', 'title': 'CEO'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the CSV lines into objects\n",
    "def csv_to_record(line):\n",
    "  parts = line.split(\",\")\n",
    "  record = {\n",
    "    \"name\": parts[0],\n",
    "    \"company\": parts[1],\n",
    "    \"title\": parts[2]\n",
    "  }\n",
    "  return record\n",
    "\n",
    "# Apply the function to every record\n",
    "records = csv_lines.map(csv_to_record)\n",
    "\n",
    "# Inspect the first item in the dataset\n",
    "records.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `pyspark.RDD.groupBy`\n",
    "\n",
    "Using the groupBy operator to count the number of jobs per person..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Florian Liebert', 'job_count': 1},\n",
       " {'name': 'Russell Jurney', 'job_count': 2},\n",
       " {'name': 'Don Brown', 'job_count': 1},\n",
       " {'name': 'Steve Jobs', 'job_count': 1},\n",
       " {'name': 'Donald Trump', 'job_count': 1}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the records by the name of the person\n",
    "grouped_records = records.groupBy(lambda x: x[\"name\"])\n",
    "\n",
    "# Show the first group\n",
    "grouped_records.first()\n",
    "\n",
    "# Count the groups\n",
    "job_counts = grouped_records.map(\n",
    "  lambda x: {\n",
    "    \"name\": x[0],\n",
    "    \"job_count\": len(x[1])\n",
    "  }\n",
    ")\n",
    "\n",
    "job_counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map vs FlatMap\n",
    "\n",
    "We need to understand the difference between the map and flatmap operators. Map groups items per-record, while flatMap creates a single large group of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Russell Jurney', 'Relato', 'CEO'],\n",
       " ['Florian Liebert', 'Mesosphere', 'CEO'],\n",
       " ['Don Brown', 'Rocana', 'CIO'],\n",
       " ['Steve Jobs', 'Apple', 'CEO'],\n",
       " ['Donald Trump', 'The Trump Organization', 'CEO'],\n",
       " ['Russell Jurney', 'Data Syndrome', 'Principal Consultant']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute a relation of words by line\n",
    "words_by_line = csv_lines\\\n",
    "  .map(lambda line: line.split(\",\"))\n",
    "\n",
    "words_by_line.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Russell Jurney',\n",
       " 'Relato',\n",
       " 'CEO',\n",
       " 'Florian Liebert',\n",
       " 'Mesosphere',\n",
       " 'CEO',\n",
       " 'Don Brown',\n",
       " 'Rocana',\n",
       " 'CIO',\n",
       " 'Steve Jobs',\n",
       " 'Apple',\n",
       " 'CEO',\n",
       " 'Donald Trump',\n",
       " 'The Trump Organization',\n",
       " 'CEO',\n",
       " 'Russell Jurney',\n",
       " 'Data Syndrome',\n",
       " 'Principal Consultant']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute a relation of words\n",
    "flattened_words = csv_lines\\\n",
    "  .map(lambda line: line.split(\",\"))\\\n",
    "  .flatMap(lambda x: x)\n",
    "\n",
    "flattened_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Rows\n",
    "\n",
    "We can create `pyspark.sql.Rows` out of python objects so you we create `pyspark.sql.DataFrames`. This is desirable because once we have `DataFrames` we can run Spark SQL on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[20] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Convert the CSV into a pyspark.sql.Row\n",
    "def csv_to_row(line):\n",
    "  parts = line.split(\",\")\n",
    "  row = Row(\n",
    "    name=parts[0],\n",
    "    company=parts[1],\n",
    "    title=parts[2]\n",
    "  )\n",
    "  return row\n",
    "\n",
    "# Apply the function to get rows in an RDD\n",
    "rows = csv_lines.map(csv_to_row)\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating `DataFrames` from `RDDs`\n",
    "\n",
    "Using the `RDD.toDF()` method to create a dataframe, registering the `DataFrame` as a temporary table with Spark SQL, and counting the jobs per person using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           name|total|\n",
      "+---------------+-----+\n",
      "|   Donald Trump|    1|\n",
      "|Florian Liebert|    1|\n",
      "|      Don Brown|    1|\n",
      "| Russell Jurney|    2|\n",
      "|     Steve Jobs|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Donald Trump', total=1),\n",
       " Row(name='Florian Liebert', total=1),\n",
       " Row(name='Don Brown', total=1),\n",
       " Row(name='Russell Jurney', total=2),\n",
       " Row(name='Steve Jobs', total=1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to a pyspark.sql.DataFrame\n",
    "rows_df = rows.toDF()\n",
    "\n",
    "# Register the DataFrame for Spark SQL\n",
    "rows_df.registerTempTable(\"executives\")\n",
    "\n",
    "# Generate a new DataFrame with SQL using the SparkSession\n",
    "job_counts = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  name,\n",
    "  COUNT(*) AS total\n",
    "  FROM executives\n",
    "  GROUP BY name\n",
    "\"\"\")\n",
    "job_counts.show()\n",
    "\n",
    "# Go back to an RDD\n",
    "job_counts.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `RDDs` from `DataFrames`\n",
    "\n",
    "We can easily convert back from a `DataFrame` to an `RDD` using the `pyspark.sql.DataFrame.rdd()` method, along with `pyspark.sql.Row.asDict()` if we desire a Python `dict` of our records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Donald Trump', 'total': 1},\n",
       " {'name': 'Florian Liebert', 'total': 1},\n",
       " {'name': 'Don Brown', 'total': 1},\n",
       " {'name': 'Russell Jurney', 'total': 2},\n",
       " {'name': 'Steve Jobs', 'total': 1}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_counts.rdd.map(lambda x: x.asDict()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Inspecting Parquet Files\n",
    "\n",
    "Using the `SparkSession` to load files as `DataFrames` and inspecting their contents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Quarter',\n",
       " 'Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'FlightDate',\n",
       " 'Carrier',\n",
       " 'TailNum',\n",
       " 'FlightNum',\n",
       " 'Origin',\n",
       " 'OriginCityName',\n",
       " 'OriginState',\n",
       " 'Dest',\n",
       " 'DestCityName',\n",
       " 'DestState',\n",
       " 'DepTime',\n",
       " 'DepDelay',\n",
       " 'DepDelayMinutes',\n",
       " 'TaxiOut',\n",
       " 'TaxiIn',\n",
       " 'WheelsOff',\n",
       " 'WheelsOn',\n",
       " 'ArrTime',\n",
       " 'ArrDelay',\n",
       " 'ArrDelayMinutes',\n",
       " 'Cancelled',\n",
       " 'Diverted',\n",
       " 'ActualElapsedTime',\n",
       " 'AirTime',\n",
       " 'Flights',\n",
       " 'Distance',\n",
       " 'CarrierDelay',\n",
       " 'WeatherDelay',\n",
       " 'NASDelay',\n",
       " 'SecurityDelay',\n",
       " 'LateAircraftDelay',\n",
       " 'CRSDepTime',\n",
       " 'CRSArrTime']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the parquet file containing flight delay records\n",
    "on_time_dataframe = spark.read.parquet('../data/on_time_performance.parquet')\n",
    "\n",
    "# Register the data for Spark SQL\n",
    "on_time_dataframe.registerTempTable(\"on_time_performance\")\n",
    "\n",
    "# Check out the columns\n",
    "on_time_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+----+-------+--------+--------+\n",
      "|FlightDate|TailNum|Origin|Dest|Carrier|DepDelay|ArrDelay|\n",
      "+----------+-------+------+----+-------+--------+--------+\n",
      "|2015-01-11| N153PQ|   ATL| GSP|     EV|    10.0|     7.0|\n",
      "|2015-01-11| N391HA|   LAX| HNL|     HA|    -3.0|   -13.0|\n",
      "|2015-01-11| N813MQ|   GSO| LGA|     MQ|   -11.0|   -21.0|\n",
      "|2015-01-13| N181UW|   LAX| CLT|     US|    48.0|    41.0|\n",
      "|2015-01-14| N337NB|   DTW| DCA|     DL|     6.0|    -4.0|\n",
      "|2015-01-15| N797AA|   JFK| LAX|     AA|    -5.0|   -64.0|\n",
      "|2015-01-15| N627AS|   SEA| MSP|     AS|    -4.0|   -16.0|\n",
      "|2015-01-15| N502NK|   MCO| LBE|     NK|     6.0|    22.0|\n",
      "|2015-01-15| N923WN|   AUS| LAX|     WN|    -5.0|     2.0|\n",
      "|2015-01-16| N325US|   MDW| ATL|     DL|     7.0|   -15.0|\n",
      "+----------+-------+------+----+-------+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trim the fields and keep the result\n",
    "trimmed_on_time = on_time_dataframe\\\n",
    "  .select(\n",
    "    \"FlightDate\",\n",
    "    \"TailNum\",\n",
    "    \"Origin\",\n",
    "    \"Dest\",\n",
    "    \"Carrier\",\n",
    "    \"DepDelay\",\n",
    "    \"ArrDelay\"\n",
    "  )\n",
    "\n",
    "# Sample 0.01% of the data and show\n",
    "trimmed_on_time.sample(False, 0.0001).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataFrame` Workflow: Dataflow to SQL and Back Again\n",
    "\n",
    "We can go back and forth between dataflow programming and SQL programming using `pyspark.sql.DataFrames`. This enables us to get the best of both worlds from these two APIs. For example, if we want to group records and get a total count for each group... a SQL `SELECT/GROUP BY/COUNT` is the most direct way to do it. On the other hand, if we want to filter data, a dataflow API call like `DataFrame.filter()` is the cleanest way. This comes down to personal preference for the user. In time you will develop your own style of working.\n",
    "\n",
    "### Calculating Velocity\n",
    "\n",
    "If we were to look at the `AirTime` along with the `Distance`, we could get a good idea of how fast the airplanes were going. Pretty cool! Lets do this using Dataflows first.\n",
    "\n",
    "#### Trimming Our Data\n",
    "\n",
    "First lets select just the two columns of interest: `AirTime` and `Distance`. We can always go back and select more columns if we want to extend our analysis, but trimming uneeded fields optimizes performance right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|AirTime|Distance|\n",
      "+-------+--------+\n",
      "|   59.0|   432.0|\n",
      "|   77.0|   432.0|\n",
      "|  129.0|   802.0|\n",
      "|   93.0|   731.0|\n",
      "|  111.0|   769.0|\n",
      "|  108.0|   769.0|\n",
      "+-------+--------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fd = on_time_dataframe.select(\"AirTime\", \"Distance\")\n",
    "fd.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Minutes to Hours\n",
    "\n",
    "Now lets convert our `AirTime` from minutes to hours by dividing by 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|             Hours|Distance|\n",
      "+------------------+--------+\n",
      "|0.9833333333333333|   432.0|\n",
      "|1.2833333333333334|   432.0|\n",
      "|              2.15|   802.0|\n",
      "|              1.55|   731.0|\n",
      "|              1.85|   769.0|\n",
      "+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hourly_fd = fd.select((fd.AirTime / 60).alias('Hours'), \"Distance\")\n",
    "hourly_fd.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Calculation\n",
    "\n",
    "Now lets calculate miles per hour!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|               Mph|\n",
      "+------------------+\n",
      "| 439.3220338983051|\n",
      "| 336.6233766233766|\n",
      "| 373.0232558139535|\n",
      "|471.61290322580646|\n",
      "| 415.6756756756757|\n",
      "|427.22222222222223|\n",
      "| 430.2739726027398|\n",
      "|              null|\n",
      "|              null|\n",
      "|              null|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "miles_per_hour = hourly_fd.select(\n",
    "    (hourly_fd.Distance / hourly_fd.Hours).alias('Mph')\n",
    ")\n",
    "miles_per_hour.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating `nulls`\n",
    "\n",
    "Looks like we have some errors in some records in our calculation because of missing fields? Lets bring back in the `Distance` and `AirTime` fields to see where the problem is coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------+\n",
      "|AirTime|             Hours|Distance|\n",
      "+-------+------------------+--------+\n",
      "|   59.0|0.9833333333333333|   432.0|\n",
      "|   77.0|1.2833333333333334|   432.0|\n",
      "|  129.0|              2.15|   802.0|\n",
      "|   93.0|              1.55|   731.0|\n",
      "|  111.0|              1.85|   769.0|\n",
      "|  108.0|               1.8|   769.0|\n",
      "|  146.0| 2.433333333333333|  1047.0|\n",
      "|   null|              null|  1007.0|\n",
      "|   null|              null|  1007.0|\n",
      "|   null|              null|   802.0|\n",
      "|   null|              null|   731.0|\n",
      "|  122.0| 2.033333333333333|   731.0|\n",
      "|   94.0|1.5666666666666667|   731.0|\n",
      "|   91.0|1.5166666666666666|   731.0|\n",
      "|  115.0|1.9166666666666667|   731.0|\n",
      "|   89.0|1.4833333333333334|   731.0|\n",
      "|  106.0|1.7666666666666666|   721.0|\n",
      "|   94.0|1.5666666666666667|   748.0|\n",
      "|   null|              null|   733.0|\n",
      "|   null|              null|   733.0|\n",
      "+-------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fd.select(\n",
    "    \"AirTime\", \n",
    "    (fd.AirTime / 60).alias('Hours'), \n",
    "    \"Distance\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering `nulls`\n",
    "\n",
    "Now that we know some records are missing `AirTimes`, we can filter those records using `pyspark.sql.DataFrame.filter()`. Starting from the beginning, lets recalculate our values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|(Distance / Hours)|\n",
      "+------------------+\n",
      "| 439.3220338983051|\n",
      "| 336.6233766233766|\n",
      "| 373.0232558139535|\n",
      "|471.61290322580646|\n",
      "| 415.6756756756757|\n",
      "|427.22222222222223|\n",
      "| 430.2739726027398|\n",
      "| 359.5081967213115|\n",
      "|466.59574468085106|\n",
      "|  481.978021978022|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fd = on_time_dataframe.select(\"AirTime\", \"Distance\")\n",
    "filled_fd = fd.filter(fd.AirTime.isNotNull())\n",
    "hourly_fd = filled_fd.select(\n",
    "    \"AirTime\", \n",
    "    (filled_fd.AirTime / 60).alias('Hours'), \n",
    "    \"Distance\"\n",
    ")\n",
    "mph = hourly_fd.select(hourly_fd.Distance / hourly_fd.Hours)\n",
    "mph.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Histograms\n",
    "\n",
    "Using `RDDs` to calculate histograms buckets and values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a histogram of departure delays\n",
    "on_time_dataframe\\\n",
    "  .select(\"DepDelay\")\\\n",
    "  .rdd\\\n",
    "  .flatMap(lambda x: x)\\\n",
    "  .histogram(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Histograms\n",
    "\n",
    "Using pyplot to visualize histograms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot a histogram using pyplot\n",
    "def create_hist(rdd_histogram_data):\n",
    "  \"\"\"Given an RDD.histogram, plot a pyplot histogram\"\"\"\n",
    "  heights = np.array(rdd_histogram_data[1])\n",
    "  full_bins = rdd_histogram_data[0]\n",
    "  mid_point_bins = full_bins[:-1]\n",
    "  widths = [abs(i - j) for i, j in zip(full_bins[:-1], full_bins[1:])]\n",
    "  bar = plt.bar(mid_point_bins, heights, width=widths, color='b')\n",
    "  return bar\n",
    "\n",
    "# Compute a histogram of departure delays\n",
    "departure_delay_histogram = on_time_dataframe\\\n",
    "  .select(\"DepDelay\")\\\n",
    "  .rdd\\\n",
    "  .flatMap(lambda x: x)\\\n",
    "  .histogram([-60,-30,-15,-10,-5,0,5,10,15,30,60,90,120,180])\n",
    "\n",
    "create_hist(departure_delay_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Airplanes in the US Fleet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the unneeded fields\n",
    "tail_numbers = on_time_dataframe.rdd.map(lambda x: x.TailNum)\n",
    "tail_numbers = tail_numbers.filter(lambda x: x != '')\n",
    "\n",
    "# distinct() gets us unique tail numbers\n",
    "unique_tail_numbers = tail_numbers.distinct()\n",
    "\n",
    "# now we need a count() of unique tail numbers\n",
    "airplane_count = unique_tail_numbers.count()\n",
    "print(\"Total airplanes: {}\".format(airplane_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting the Total Flights Per Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SQL to look at the total flights by month across 2015\n",
    "on_time_dataframe.registerTempTable(\"on_time_dataframe\")\n",
    "total_flights_by_month = spark.sql(\n",
    "  \"\"\"SELECT Month, Year, COUNT(*) AS total_flights\n",
    "  FROM on_time_dataframe\n",
    "  GROUP BY Year, Month\n",
    "  ORDER BY Year, Month\"\"\"\n",
    ")\n",
    "\n",
    "# This map/asDict trick makes the rows print a little prettier. It is optional.\n",
    "flights_chart_data = total_flights_by_month.rdd.map(lambda row: row.asDict())\n",
    "flights_chart_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `RDDs` and Map/Reduce to Prepare a Complex Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to the fields we need to identify and link to a flight\n",
    "flights = on_time_dataframe.rdd.map(lambda x: \n",
    "  (x.Carrier, x.FlightDate, x.FlightNum, x.Origin, x.Dest, x.TailNum)\n",
    "  )\n",
    "\n",
    "# Group flights by tail number, sorted by date, then flight number, then origin/dest\n",
    "flights_per_airplane = flights\\\n",
    "  .map(lambda nameTuple: (nameTuple[5], [nameTuple[0:5]]))\\\n",
    "  .reduceByKey(lambda a, b: a + b)\\\n",
    "  .map(lambda tuple:\n",
    "      {\n",
    "        'TailNum': tuple[0], \n",
    "        'Flights': sorted(tuple[1], key=lambda x: (x[1], x[2], x[3], x[4]))\n",
    "      }\n",
    "    )\n",
    "flights_per_airplane.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Late Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_flights = on_time_dataframe.count()\n",
    "\n",
    "# Flights that were late leaving...\n",
    "late_departures = on_time_dataframe.filter(\n",
    "  on_time_dataframe.DepDelayMinutes > 0\n",
    ")\n",
    "total_late_departures = late_departures.count()\n",
    "print(total_late_departures)\n",
    "\n",
    "# Flights that were late arriving...\n",
    "late_arrivals = on_time_dataframe.filter(\n",
    "  on_time_dataframe.ArrDelayMinutes > 0\n",
    ")\n",
    "total_late_arrivals = late_arrivals.count()\n",
    "print(total_late_arrivals)\n",
    "\n",
    "# Get the percentage of flights that are late, rounded to 1 decimal place\n",
    "pct_late = round((total_late_arrivals / (total_flights * 1.0)) * 100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Flights with Hero Captains\n",
    "\n",
    "\"Hero Captains\" are those that depart late but make up time in the air and arrive on time or early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flights that left late but made up time to arrive on time...\n",
    "on_time_heros = on_time_dataframe.filter(\n",
    "  (on_time_dataframe.DepDelayMinutes > 0)\n",
    "  &\n",
    "  (on_time_dataframe.ArrDelayMinutes <= 0)\n",
    ")\n",
    "total_on_time_heros = on_time_heros.count()\n",
    "print(total_on_time_heros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing Our Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total flights:   {:,}\".format(total_flights))\n",
    "print(\"Late departures: {:,}\".format(total_late_departures))\n",
    "print(\"Late arrivals:   {:,}\".format(total_late_arrivals))\n",
    "print(\"Recoveries:      {:,}\".format(total_on_time_heros))\n",
    "print(\"Percentage Late: {}%\".format(pct_late))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Average Lateness Per Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average minutes late departing and arriving\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  ROUND(AVG(DepDelay),1) AS AvgDepDelay,\n",
    "  ROUND(AVG(ArrDelay),1) AS AvgArrDelay\n",
    "FROM on_time_performance\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Late Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are flights late? Lets look at some delayed flights and the delay causes\n",
    "late_flights = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  ArrDelayMinutes,\n",
    "  WeatherDelay,\n",
    "  CarrierDelay,\n",
    "  NASDelay,\n",
    "  SecurityDelay,\n",
    "  LateAircraftDelay\n",
    "FROM\n",
    "  on_time_performance\n",
    "WHERE\n",
    "  WeatherDelay IS NOT NULL\n",
    "  OR\n",
    "  CarrierDelay IS NOT NULL\n",
    "  OR\n",
    "  NASDelay IS NOT NULL\n",
    "  OR\n",
    "  SecurityDelay IS NOT NULL\n",
    "  OR\n",
    "  LateAircraftDelay IS NOT NULL\n",
    "ORDER BY\n",
    "  FlightDate\n",
    "\"\"\")\n",
    "late_flights.sample(False, 0.01).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Why Flights Are Late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage contribution to delay for each source\n",
    "total_delays = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  ROUND(SUM(WeatherDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_weather_delay,\n",
    "  ROUND(SUM(CarrierDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_carrier_delay,\n",
    "  ROUND(SUM(NASDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_nas_delay,\n",
    "  ROUND(SUM(SecurityDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_security_delay,\n",
    "  ROUND(SUM(LateAircraftDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_late_aircraft_delay\n",
    "FROM on_time_performance\n",
    "\"\"\")\n",
    "total_delays.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing a Histogram of Weather Delayed Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eyeball the first to define our buckets\n",
    "weather_delay_histogram = on_time_dataframe\\\n",
    "  .select(\"WeatherDelay\")\\\n",
    "  .rdd\\\n",
    "  .flatMap(lambda x: x)\\\n",
    "  .histogram([1, 5, 10, 15, 30, 60, 120, 240, 480, 720, 24*60.0])\n",
    "print(weather_delay_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hist(weather_delay_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a Histogram for Visualization by d3.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data into something easily consumed by d3\n",
    "def histogram_to_publishable(histogram):\n",
    "  record = {'key': 1, 'data': []}\n",
    "  for label, value in zip(histogram[0], histogram[1]):\n",
    "    record['data'].append(\n",
    "      {\n",
    "        'label': label,\n",
    "        'value': value\n",
    "      }\n",
    "    )\n",
    "  return record\n",
    "\n",
    "# Recompute the weather histogram with a filter for on-time flights\n",
    "weather_delay_histogram = on_time_dataframe\\\n",
    "  .filter(\n",
    "    (on_time_dataframe.WeatherDelay != None)\n",
    "    &\n",
    "    (on_time_dataframe.WeatherDelay > 0)\n",
    "  )\\\n",
    "  .select(\"WeatherDelay\")\\\n",
    "  .rdd\\\n",
    "  .flatMap(lambda x: x)\\\n",
    "  .histogram([0, 15, 30, 60, 120, 240, 480, 720, 24*60.0])\n",
    "print(weather_delay_histogram)\n",
    "\n",
    "record = histogram_to_publishable(weather_delay_histogram)\n",
    "record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Classifier Model to Predict Flight Delays\n",
    "\n",
    "## Loading Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, DateType, TimestampType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"ArrDelay\", DoubleType(), True),     # \"ArrDelay\":5.0\n",
    "  StructField(\"CRSArrTime\", TimestampType(), True),    # \"CRSArrTime\":\"2015-12-31T03:20:00.000-08:00\"\n",
    "  StructField(\"CRSDepTime\", TimestampType(), True),    # \"CRSDepTime\":\"2015-12-31T03:05:00.000-08:00\"\n",
    "  StructField(\"Carrier\", StringType(), True),     # \"Carrier\":\"WN\"\n",
    "  StructField(\"DayOfMonth\", IntegerType(), True), # \"DayOfMonth\":31\n",
    "  StructField(\"DayOfWeek\", IntegerType(), True),  # \"DayOfWeek\":4\n",
    "  StructField(\"DayOfYear\", IntegerType(), True),  # \"DayOfYear\":365\n",
    "  StructField(\"DepDelay\", DoubleType(), True),     # \"DepDelay\":14.0\n",
    "  StructField(\"Dest\", StringType(), True),        # \"Dest\":\"SAN\"\n",
    "  StructField(\"Distance\", DoubleType(), True),     # \"Distance\":368.0\n",
    "  StructField(\"FlightDate\", DateType(), True),    # \"FlightDate\":\"2015-12-30T16:00:00.000-08:00\"\n",
    "  StructField(\"FlightNum\", StringType(), True),   # \"FlightNum\":\"6109\"\n",
    "  StructField(\"Origin\", StringType(), True),      # \"Origin\":\"TUS\"\n",
    "])\n",
    "\n",
    "features = spark.read.json(\n",
    "  \"../data/simple_flight_delay_features.jsonl.bz2\",\n",
    "  schema=schema\n",
    ")\n",
    "features.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data for Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Check for nulls in features before using Spark ML\n",
    "#\n",
    "null_counts = [(column, features.where(features[column].isNull()).count()) for column in features.columns]\n",
    "cols_with_nulls = filter(lambda x: x[1] > 0, null_counts)\n",
    "print(list(cols_with_nulls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a Route Column\n",
    "\n",
    "Demonstrating the addition of a feature to our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Add a Route variable to replace FlightNum\n",
    "#\n",
    "from pyspark.sql.functions import lit, concat\n",
    "\n",
    "features_with_route = features.withColumn(\n",
    "  'Route',\n",
    "  concat(\n",
    "    features.Origin,\n",
    "    lit('-'),\n",
    "    features.Dest\n",
    "  )\n",
    ")\n",
    "features_with_route.select(\"Origin\", \"Dest\", \"Route\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketizing ArrDelay into ArrDelayBucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Use pysmark.ml.feature.Bucketizer to bucketize ArrDelay\n",
    "#\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), -15.0, 0, 30.0, float(\"inf\")]\n",
    "bucketizer = Bucketizer(\n",
    "  splits=splits,\n",
    "  inputCol=\"ArrDelay\",\n",
    "  outputCol=\"ArrDelayBucket\"\n",
    ")\n",
    "ml_bucketized_features = bucketizer.transform(features_with_route)\n",
    "\n",
    "# Check the buckets out\n",
    "ml_bucketized_features.select(\"ArrDelay\", \"ArrDelayBucket\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Our String Fields into Numeric Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Extract features tools in with pyspark.ml.feature\n",
    "#\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "# Turn category fields into categoric feature vectors, then drop intermediate fields\n",
    "for column in [\"Carrier\", \"DayOfMonth\", \"DayOfWeek\", \"DayOfYear\",\n",
    "               \"Origin\", \"Dest\", \"Route\"]:\n",
    "  string_indexer = StringIndexer(\n",
    "    inputCol=column,\n",
    "    outputCol=column + \"_index\"\n",
    "  )\n",
    "  ml_bucketized_features = string_indexer.fit(ml_bucketized_features)\\\n",
    "                                          .transform(ml_bucketized_features)\n",
    "\n",
    "# Check out the indexes\n",
    "ml_bucketized_features.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Numeric Fields into a Single Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle continuous, numeric fields by combining them into one feature vector\n",
    "numeric_columns = [\"DepDelay\", \"Distance\"]\n",
    "index_columns = [\"Carrier_index\", \"DayOfMonth_index\",\n",
    "                   \"DayOfWeek_index\", \"DayOfYear_index\", \"Origin_index\",\n",
    "                   \"Origin_index\", \"Dest_index\", \"Route_index\"]\n",
    "vector_assembler = VectorAssembler(\n",
    "  inputCols=numeric_columns + index_columns,\n",
    "  outputCol=\"Features_vec\"\n",
    ")\n",
    "final_vectorized_features = vector_assembler.transform(ml_bucketized_features)\n",
    "\n",
    "# Drop the index columns\n",
    "for column in index_columns:\n",
    "  final_vectorized_features = final_vectorized_features.drop(column)\n",
    "\n",
    "# Check out the features\n",
    "final_vectorized_features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Our Model in an Experimental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Cross validate, train and evaluate classifier\n",
    "#\n",
    "\n",
    "# Test/train split\n",
    "training_data, test_data = final_vectorized_features.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Instantiate and fit random forest classifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(\n",
    "  featuresCol=\"Features_vec\",\n",
    "  labelCol=\"ArrDelayBucket\",\n",
    "  maxBins=4657,\n",
    "  maxMemoryInMB=1024\n",
    ")\n",
    "model = rfc.fit(training_data)\n",
    "\n",
    "# Evaluate model using test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = {}\".format(accuracy))\n",
    "\n",
    "# Check a sample\n",
    "predictions.sample(False, 0.001, 18).orderBy(\"CRSDepTime\").show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
