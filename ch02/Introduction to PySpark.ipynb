{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize PySpark\n",
    "\n",
    "First, we use the findspark package to initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark initiated...\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark\n",
    "APP_NAME = \"Debugging Prediction Problems\"\n",
    "\n",
    "# If there is no SparkSession, create the environment\n",
    "try:\n",
    "  sc and spark\n",
    "except NameError as e:\n",
    "  import findspark\n",
    "  findspark.init()\n",
    "  import pyspark\n",
    "  import pyspark.sql\n",
    "\n",
    "  sc = pyspark.SparkContext()\n",
    "  spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "print(\"PySpark initiated...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, World!\n",
    "\n",
    "Loading data, mapping it and collecting the records into RAM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Russell Jurney', 'Relato', 'CEO'],\n",
       " ['Florian Liebert', 'Mesosphere', 'CEO'],\n",
       " ['Don Brown', 'Rocana', 'CIO'],\n",
       " ['Steve Jobs', 'Apple', 'CEO'],\n",
       " ['Donald Trump', 'The Trump Organization', 'CEO'],\n",
       " ['Russell Jurney', 'Data Syndrome', 'Principal Consultant']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text file using the SparkContext\n",
    "csv_lines = sc.textFile(\"../data/example.csv\")\n",
    "\n",
    "# Map the data to split the lines into a list\n",
    "data = csv_lines.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Collect the dataset into local RAM\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Objects from CSV using `pyspark.RDD.map`\n",
    "\n",
    "Using a function with a map operation to create objects (dicts) as records..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Russell Jurney', 'company': 'Relato', 'title': 'CEO'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the CSV lines into objects\n",
    "def csv_to_record(line):\n",
    "  parts = line.split(\",\")\n",
    "  record = {\n",
    "    \"name\": parts[0],\n",
    "    \"company\": parts[1],\n",
    "    \"title\": parts[2]\n",
    "  }\n",
    "  return record\n",
    "\n",
    "# Apply the function to every record\n",
    "records = csv_lines.map(csv_to_record)\n",
    "\n",
    "# Inspect the first item in the dataset\n",
    "records.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `pyspark.RDD.groupBy`\n",
    "\n",
    "Using the groupBy operator to count the number of jobs per person..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Florian Liebert', 'job_count': 1},\n",
       " {'name': 'Russell Jurney', 'job_count': 2},\n",
       " {'name': 'Don Brown', 'job_count': 1},\n",
       " {'name': 'Steve Jobs', 'job_count': 1},\n",
       " {'name': 'Donald Trump', 'job_count': 1}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the records by the name of the person\n",
    "grouped_records = records.groupBy(lambda x: x[\"name\"])\n",
    "\n",
    "# Show the first group\n",
    "grouped_records.first()\n",
    "\n",
    "# Count the groups\n",
    "job_counts = grouped_records.map(\n",
    "  lambda x: {\n",
    "    \"name\": x[0],\n",
    "    \"job_count\": len(x[1])\n",
    "  }\n",
    ")\n",
    "\n",
    "job_counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map vs FlatMap\n",
    "\n",
    "We need to understand the difference between the map and flatmap operators. Map groups items per-record, while flatMap creates a single large group of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Russell Jurney', 'Relato', 'CEO'],\n",
       " ['Florian Liebert', 'Mesosphere', 'CEO'],\n",
       " ['Don Brown', 'Rocana', 'CIO'],\n",
       " ['Steve Jobs', 'Apple', 'CEO'],\n",
       " ['Donald Trump', 'The Trump Organization', 'CEO'],\n",
       " ['Russell Jurney', 'Data Syndrome', 'Principal Consultant']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute a relation of words by line\n",
    "words_by_line = csv_lines\\\n",
    "  .map(lambda line: line.split(\",\"))\n",
    "\n",
    "words_by_line.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Russell Jurney',\n",
       " 'Relato',\n",
       " 'CEO',\n",
       " 'Florian Liebert',\n",
       " 'Mesosphere',\n",
       " 'CEO',\n",
       " 'Don Brown',\n",
       " 'Rocana',\n",
       " 'CIO',\n",
       " 'Steve Jobs',\n",
       " 'Apple',\n",
       " 'CEO',\n",
       " 'Donald Trump',\n",
       " 'The Trump Organization',\n",
       " 'CEO',\n",
       " 'Russell Jurney',\n",
       " 'Data Syndrome',\n",
       " 'Principal Consultant']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute a relation of words\n",
    "flattened_words = csv_lines\\\n",
    "  .map(lambda line: line.split(\",\"))\\\n",
    "  .flatMap(lambda x: x)\n",
    "\n",
    "flattened_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Rows\n",
    "\n",
    "We can create `pyspark.sql.Rows` out of python objects so you we create `pyspark.sql.DataFrames`. This is desirable because once we have `DataFrames` we can run Spark SQL on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[20] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Convert the CSV into a pyspark.sql.Row\n",
    "def csv_to_row(line):\n",
    "  parts = line.split(\",\")\n",
    "  row = Row(\n",
    "    name=parts[0],\n",
    "    company=parts[1],\n",
    "    title=parts[2]\n",
    "  )\n",
    "  return row\n",
    "\n",
    "# Apply the function to get rows in an RDD\n",
    "rows = csv_lines.map(csv_to_row)\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating `DataFrames` from `RDDs`\n",
    "\n",
    "Using the `RDD.toDF()` method to create a dataframe, registering the `DataFrame` as a temporary table with Spark SQL, and counting the jobs per person using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           name|total|\n",
      "+---------------+-----+\n",
      "|   Donald Trump|    1|\n",
      "|Florian Liebert|    1|\n",
      "|      Don Brown|    1|\n",
      "| Russell Jurney|    2|\n",
      "|     Steve Jobs|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Donald Trump', total=1),\n",
       " Row(name='Florian Liebert', total=1),\n",
       " Row(name='Don Brown', total=1),\n",
       " Row(name='Russell Jurney', total=2),\n",
       " Row(name='Steve Jobs', total=1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to a pyspark.sql.DataFrame\n",
    "rows_df = rows.toDF()\n",
    "\n",
    "# Register the DataFrame for Spark SQL\n",
    "rows_df.registerTempTable(\"executives\")\n",
    "\n",
    "# Generate a new DataFrame with SQL using the SparkSession\n",
    "job_counts = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  name,\n",
    "  COUNT(*) AS total\n",
    "  FROM executives\n",
    "  GROUP BY name\n",
    "\"\"\")\n",
    "job_counts.show()\n",
    "\n",
    "# Go back to an RDD\n",
    "job_counts.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `RDDs` from `DataFrames`\n",
    "\n",
    "We can easily convert back from a `DataFrame` to an `RDD` using the `pyspark.sql.DataFrame.rdd()` method, along with `pyspark.sql.Row.asDict()` if we desire a Python `dict` of our records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Donald Trump', 'total': 1},\n",
       " {'name': 'Florian Liebert', 'total': 1},\n",
       " {'name': 'Don Brown', 'total': 1},\n",
       " {'name': 'Russell Jurney', 'total': 2},\n",
       " {'name': 'Steve Jobs', 'total': 1}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_counts.rdd.map(lambda x: x.asDict()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Inspecting Parquet Files\n",
    "\n",
    "Using the `SparkSession` to load files as `DataFrames` and inspecting their contents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Quarter',\n",
       " 'Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'FlightDate',\n",
       " 'Carrier',\n",
       " 'TailNum',\n",
       " 'FlightNum',\n",
       " 'Origin',\n",
       " 'OriginCityName',\n",
       " 'OriginState',\n",
       " 'Dest',\n",
       " 'DestCityName',\n",
       " 'DestState',\n",
       " 'DepTime',\n",
       " 'DepDelay',\n",
       " 'DepDelayMinutes',\n",
       " 'TaxiOut',\n",
       " 'TaxiIn',\n",
       " 'WheelsOff',\n",
       " 'WheelsOn',\n",
       " 'ArrTime',\n",
       " 'ArrDelay',\n",
       " 'ArrDelayMinutes',\n",
       " 'Cancelled',\n",
       " 'Diverted',\n",
       " 'ActualElapsedTime',\n",
       " 'AirTime',\n",
       " 'Flights',\n",
       " 'Distance',\n",
       " 'CarrierDelay',\n",
       " 'WeatherDelay',\n",
       " 'NASDelay',\n",
       " 'SecurityDelay',\n",
       " 'LateAircraftDelay',\n",
       " 'CRSDepTime',\n",
       " 'CRSArrTime']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the parquet file containing flight delay records\n",
    "on_time_dataframe = spark.read.parquet('../data/on_time_performance.parquet')\n",
    "\n",
    "# Register the data for Spark SQL\n",
    "on_time_dataframe.registerTempTable(\"on_time_performance\")\n",
    "\n",
    "# Check out the columns\n",
    "on_time_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+----+-------+--------+--------+\n",
      "|FlightDate|TailNum|Origin|Dest|Carrier|DepDelay|ArrDelay|\n",
      "+----------+-------+------+----+-------+--------+--------+\n",
      "|2015-01-11| N153PQ|   ATL| GSP|     EV|    10.0|     7.0|\n",
      "|2015-01-11| N391HA|   LAX| HNL|     HA|    -3.0|   -13.0|\n",
      "|2015-01-11| N813MQ|   GSO| LGA|     MQ|   -11.0|   -21.0|\n",
      "|2015-01-13| N181UW|   LAX| CLT|     US|    48.0|    41.0|\n",
      "|2015-01-14| N337NB|   DTW| DCA|     DL|     6.0|    -4.0|\n",
      "|2015-01-15| N797AA|   JFK| LAX|     AA|    -5.0|   -64.0|\n",
      "|2015-01-15| N627AS|   SEA| MSP|     AS|    -4.0|   -16.0|\n",
      "|2015-01-15| N502NK|   MCO| LBE|     NK|     6.0|    22.0|\n",
      "|2015-01-15| N923WN|   AUS| LAX|     WN|    -5.0|     2.0|\n",
      "|2015-01-16| N325US|   MDW| ATL|     DL|     7.0|   -15.0|\n",
      "+----------+-------+------+----+-------+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trim the fields and keep the result\n",
    "trimmed_on_time = on_time_dataframe\\\n",
    "  .select(\n",
    "    \"FlightDate\",\n",
    "    \"TailNum\",\n",
    "    \"Origin\",\n",
    "    \"Dest\",\n",
    "    \"Carrier\",\n",
    "    \"DepDelay\",\n",
    "    \"ArrDelay\"\n",
    "  )\n",
    "\n",
    "# Sample 0.01% of the data and show\n",
    "trimmed_on_time.sample(False, 0.0001).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataFrame` Workflow: Calculating Speed in Dataflow and SQL\n",
    "\n",
    "We can go back and forth between dataflow programming and SQL programming using `pyspark.sql.DataFrames`. This enables us to get the best of both worlds from these two APIs. For example, if we want to group records and get a total count for each group... a SQL `SELECT/GROUP BY/COUNT` is the most direct way to do it. On the other hand, if we want to filter data, a dataflow API call like `DataFrame.filter()` is the cleanest way. This comes down to personal preference for the user. In time you will develop your own style of working.\n",
    "\n",
    "### Dataflow Programming\n",
    "\n",
    "If we were to look at the `AirTime` along with the `Distance`, we could get a good idea of how fast the airplanes were going. Pretty cool! Lets do this using Dataflows first.\n",
    "\n",
    "#### Trimming Our Data\n",
    "\n",
    "First lets select just the two columns of interest: `AirTime` and `Distance`. We can always go back and select more columns if we want to extend our analysis, but trimming uneeded fields optimizes performance right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|AirTime|Distance|\n",
      "+-------+--------+\n",
      "|   59.0|   432.0|\n",
      "|   77.0|   432.0|\n",
      "|  129.0|   802.0|\n",
      "|   93.0|   731.0|\n",
      "|  111.0|   769.0|\n",
      "|  108.0|   769.0|\n",
      "+-------+--------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fd = on_time_dataframe.select(\"AirTime\", \"Distance\")\n",
    "fd.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Minutes to Hours\n",
    "\n",
    "Now lets convert our `AirTime` from minutes to hours by dividing by 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|             Hours|Distance|\n",
      "+------------------+--------+\n",
      "|0.9833333333333333|   432.0|\n",
      "|1.2833333333333334|   432.0|\n",
      "|              2.15|   802.0|\n",
      "|              1.55|   731.0|\n",
      "|              1.85|   769.0|\n",
      "+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hourly_fd = fd.select((fd.AirTime / 60).alias('Hours'), \"Distance\")\n",
    "hourly_fd.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Calculation\n",
    "\n",
    "Now lets calculate miles per hour!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|               Mph|\n",
      "+------------------+\n",
      "| 439.3220338983051|\n",
      "| 336.6233766233766|\n",
      "| 373.0232558139535|\n",
      "|471.61290322580646|\n",
      "| 415.6756756756757|\n",
      "|427.22222222222223|\n",
      "| 430.2739726027398|\n",
      "|              null|\n",
      "|              null|\n",
      "|              null|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "miles_per_hour = hourly_fd.select(\n",
    "    (hourly_fd.Distance / hourly_fd.Hours).alias('Mph')\n",
    ")\n",
    "miles_per_hour.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating `nulls`\n",
    "\n",
    "Looks like we have some errors in some records in our calculation because of missing fields? Lets bring back in the `Distance` and `AirTime` fields to see where the problem is coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------+\n",
      "|AirTime|             Hours|Distance|\n",
      "+-------+------------------+--------+\n",
      "|   59.0|0.9833333333333333|   432.0|\n",
      "|   77.0|1.2833333333333334|   432.0|\n",
      "|  129.0|              2.15|   802.0|\n",
      "|   93.0|              1.55|   731.0|\n",
      "|  111.0|              1.85|   769.0|\n",
      "|  108.0|               1.8|   769.0|\n",
      "|  146.0| 2.433333333333333|  1047.0|\n",
      "|   null|              null|  1007.0|\n",
      "|   null|              null|  1007.0|\n",
      "|   null|              null|   802.0|\n",
      "|   null|              null|   731.0|\n",
      "|  122.0| 2.033333333333333|   731.0|\n",
      "|   94.0|1.5666666666666667|   731.0|\n",
      "|   91.0|1.5166666666666666|   731.0|\n",
      "|  115.0|1.9166666666666667|   731.0|\n",
      "|   89.0|1.4833333333333334|   731.0|\n",
      "|  106.0|1.7666666666666666|   721.0|\n",
      "|   94.0|1.5666666666666667|   748.0|\n",
      "|   null|              null|   733.0|\n",
      "|   null|              null|   733.0|\n",
      "+-------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fd.select(\n",
    "    \"AirTime\", \n",
    "    (fd.AirTime / 60).alias('Hours'), \n",
    "    \"Distance\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering `nulls`\n",
    "\n",
    "Now that we know some records are missing `AirTimes`, we can filter those records using `pyspark.sql.DataFrame.filter()`. Starting from the beginning, lets recalculate our values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|               Mph|\n",
      "+------------------+\n",
      "| 439.3220338983051|\n",
      "| 336.6233766233766|\n",
      "| 373.0232558139535|\n",
      "|471.61290322580646|\n",
      "| 415.6756756756757|\n",
      "|427.22222222222223|\n",
      "| 430.2739726027398|\n",
      "| 359.5081967213115|\n",
      "|466.59574468085106|\n",
      "|  481.978021978022|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fd = on_time_dataframe.select(\"AirTime\", \"Distance\")\n",
    "filled_fd = fd.filter(fd.AirTime.isNotNull())\n",
    "hourly_fd = filled_fd.select(\n",
    "    \"AirTime\", \n",
    "    (filled_fd.AirTime / 60).alias('Hours'), \n",
    "    \"Distance\"\n",
    ")\n",
    "mph = hourly_fd.select((hourly_fd.Distance / hourly_fd.Hours).alias('Mph'))\n",
    "mph.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaging Speed\n",
    "\n",
    "How fast does the fleet travel overall? Lets compute the average speed for the entire fleet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          avg(Mph)|\n",
      "+------------------+\n",
      "|408.72370268222824|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mph.select(\n",
    "    pyspark.sql.functions.avg(mph.Mph)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the average speed of the fleet is 408 mph. Note how along the way we chekced the data for sanity, which led to confidence in our answer. SQL by contast can hide the internals of a query, which might have skewed our average significantly! \n",
    "\n",
    "### SQL-Based Speed Calculation\n",
    "\n",
    "Now lets work the same thing out in SQL. Starting from the top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|               Mph|\n",
      "+------------------+\n",
      "| 439.3220338983051|\n",
      "| 336.6233766233766|\n",
      "| 373.0232558139535|\n",
      "|471.61290322580646|\n",
      "| 415.6756756756757|\n",
      "|427.22222222222223|\n",
      "| 430.2739726027398|\n",
      "| 359.5081967213115|\n",
      "|466.59574468085106|\n",
      "|  481.978021978022|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "on_time_dataframe.registerTempTable(\"on_time_performance\")\n",
    "\n",
    "mph = spark.sql(\"\"\"\n",
    "SELECT ( Distance / ( AirTime/60 ) ) AS Mph \n",
    "FROM on_time_performance \n",
    "WHERE AirTime IS NOT NULL\n",
    "\"\"\")\n",
    "mph.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating SQL\n",
    "\n",
    "The SQL based solution seems to be better in this case, because we can simply express our calculation all in one place. When complexity grows however, it is best to break a single query into multiple stages where you use SQL or Dataflow programming to massage the data into shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Histograms\n",
    "\n",
    "Having computed the speed in miles per hour and the overall average speed of passenger jets in the US, lets dig deeper by using the `RDD` API's histogram method to calculate histograms buckets and values, which will then use to visualize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([32.34375,\n",
       "  107.89626024590164,\n",
       "  183.4487704918033,\n",
       "  259.0012807377049,\n",
       "  334.5537909836066,\n",
       "  410.10630122950823,\n",
       "  485.65881147540983,\n",
       "  561.2113217213115,\n",
       "  636.7638319672132,\n",
       "  712.3163422131148,\n",
       "  787.8688524590165],\n",
       " [174, 14255, 148314, 643901, 1861699, 2357002, 664609, 23997, 32, 25])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute a histogram of departure delays\n",
    "mph\\\n",
    "  .select(\"Mph\")\\\n",
    "  .rdd\\\n",
    "  .flatMap(lambda x: x)\\\n",
    "  .histogram(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Histograms\n",
    "\n",
    "The problem with the output above is that it is hard to interpret. For better understanding, we need a visualization. We can use `matplotlib` inline in a Jupyter Notebook to visualize this distribution and see what the tendency of speed of airplanes is around the mean of 408 mph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot a histogram using pyplot\n",
    "def create_hist(rdd_histogram_data):\n",
    "  \"\"\"Given an RDD.histogram, plot a pyplot histogram\"\"\"\n",
    "  heights = np.array(rdd_histogram_data[1])\n",
    "  full_bins = rdd_histogram_data[0]\n",
    "  mid_point_bins = full_bins[:-1]\n",
    "  widths = [abs(i - j) for i, j in zip(full_bins[:-1], full_bins[1:])]\n",
    "  bar = plt.bar(mid_point_bins, heights, width=widths, color='b')\n",
    "  return bar\n",
    "\n",
    "# Compute a histogram of departure delays\n",
    "departure_delay_histogram = mph\\\n",
    "  .select(\"Mph\")\\\n",
    "  .rdd\\\n",
    "  .flatMap(lambda x: x)\\\n",
    "  .histogram(10)\n",
    "\n",
    "create_hist(departure_delay_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating on a Histogram\n",
    "\n",
    "That looks interesting, but the bars seem to fat the really see what is going on. Lets double the number of buckets from 10 to 20. We can reuse the `create_hist()` method to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 20 artists>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute a histogram of departure delays\n",
    "departure_delay_histogram = mph\\\n",
    "  .select(\"Mph\")\\\n",
    "  .rdd\\\n",
    "  .flatMap(lambda x: x)\\\n",
    "  .histogram(20)\n",
    "\n",
    "create_hist(departure_delay_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Summary\n",
    "\n",
    "You've now seen how to calculate different values in both SQL and Dataflow style, how to switch between the two methods, how to switch between the `pyspark.RDD` and `pyspark.sql.DataFrame` APIs and you're starting to build a proficiency in PySpark!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Airplanes in the US Fleet\n",
    "\n",
    "Lets convert our `on_time_dataframe` (a `DataFrame`) into an `RDD` to calculate the total number of airplanes in the US fleet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total airplanes: 4897\n"
     ]
    }
   ],
   "source": [
    "# Dump the unneeded fields\n",
    "tail_numbers = on_time_dataframe.rdd.map(lambda x: x.TailNum)\n",
    "tail_numbers = tail_numbers.filter(lambda x: x != '' and x is not None)\n",
    "\n",
    "# distinct() gets us unique tail numbers\n",
    "unique_tail_numbers = tail_numbers.distinct()\n",
    "\n",
    "# now we need a count() of unique tail numbers\n",
    "airplane_count = unique_tail_numbers.count()\n",
    "print(\"Total airplanes: {}\".format(airplane_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Characterizing Airports\n",
    "\n",
    "Using the techniques we demonstated above, calculate any 3 out of 4 of the following things using both the SQL and the Dataflow methods for each one. That is: implement each calculation twice - once in SQL and once using Dataflows. Try to use both the `RDD` and `DataFrame` APIs as you work.\n",
    "\n",
    "1. How many airports are there in the united states?\n",
    "2. What is the average flight time for flights arriving in San Francisco (SFO)? What does the distribution of this value look like? Plot a histogram using the `create_hist` method shown above.\n",
    "3. Which American airport has the fastest out-bound speeds? What does the distribution of the flight speeds at this one airport look like? Plot a histogram using the `create_hist` method shown above.\n",
    "4. What were the worst travel dates in terms of overall delayed flights in the US in 2015?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Quarter',\n",
       " 'Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'FlightDate',\n",
       " 'Carrier',\n",
       " 'TailNum',\n",
       " 'FlightNum',\n",
       " 'Origin',\n",
       " 'OriginCityName',\n",
       " 'OriginState',\n",
       " 'Dest',\n",
       " 'DestCityName',\n",
       " 'DestState',\n",
       " 'DepTime',\n",
       " 'DepDelay',\n",
       " 'DepDelayMinutes',\n",
       " 'TaxiOut',\n",
       " 'TaxiIn',\n",
       " 'WheelsOff',\n",
       " 'WheelsOn',\n",
       " 'ArrTime',\n",
       " 'ArrDelay',\n",
       " 'ArrDelayMinutes',\n",
       " 'Cancelled',\n",
       " 'Diverted',\n",
       " 'ActualElapsedTime',\n",
       " 'AirTime',\n",
       " 'Flights',\n",
       " 'Distance',\n",
       " 'CarrierDelay',\n",
       " 'WeatherDelay',\n",
       " 'NASDelay',\n",
       " 'SecurityDelay',\n",
       " 'LateAircraftDelay',\n",
       " 'CRSDepTime',\n",
       " 'CRSArrTime']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_time_dataframe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `RDDs` and Map/Reduce to Prepare a Complex Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to the fields we need to identify and link to a flight\n",
    "flights = on_time_dataframe.rdd.map(lambda x: \n",
    "  (x.Carrier, x.FlightDate, x.FlightNum, x.Origin, x.Dest, x.TailNum)\n",
    "  )\n",
    "\n",
    "# Group flights by tail number, sorted by date, then flight number, then origin/dest\n",
    "flights_per_airplane = flights\\\n",
    "  .map(lambda nameTuple: (nameTuple[5], [nameTuple[0:5]]))\\\n",
    "  .reduceByKey(lambda a, b: a + b)\\\n",
    "  .map(lambda tuple:\n",
    "      {\n",
    "        'TailNum': tuple[0], \n",
    "        'Flights': sorted(tuple[1], key=lambda x: (x[1], x[2], x[3], x[4]))\n",
    "      }\n",
    "    )\n",
    "flights_per_airplane.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Late Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_flights = on_time_dataframe.count()\n",
    "\n",
    "# Flights that were late leaving...\n",
    "late_departures = on_time_dataframe.filter(\n",
    "  on_time_dataframe.DepDelayMinutes > 0\n",
    ")\n",
    "total_late_departures = late_departures.count()\n",
    "print(total_late_departures)\n",
    "\n",
    "# Flights that were late arriving...\n",
    "late_arrivals = on_time_dataframe.filter(\n",
    "  on_time_dataframe.ArrDelayMinutes > 0\n",
    ")\n",
    "total_late_arrivals = late_arrivals.count()\n",
    "print(total_late_arrivals)\n",
    "\n",
    "# Get the percentage of flights that are late, rounded to 1 decimal place\n",
    "pct_late = round((total_late_arrivals / (total_flights * 1.0)) * 100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Flights with Hero Captains\n",
    "\n",
    "\"Hero Captains\" are those that depart late but make up time in the air and arrive on time or early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flights that left late but made up time to arrive on time...\n",
    "on_time_heros = on_time_dataframe.filter(\n",
    "  (on_time_dataframe.DepDelayMinutes > 0)\n",
    "  &\n",
    "  (on_time_dataframe.ArrDelayMinutes <= 0)\n",
    ")\n",
    "total_on_time_heros = on_time_heros.count()\n",
    "print(total_on_time_heros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing Our Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total flights:   {:,}\".format(total_flights))\n",
    "print(\"Late departures: {:,}\".format(total_late_departures))\n",
    "print(\"Late arrivals:   {:,}\".format(total_late_arrivals))\n",
    "print(\"Recoveries:      {:,}\".format(total_on_time_heros))\n",
    "print(\"Percentage Late: {}%\".format(pct_late))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Average Lateness Per Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average minutes late departing and arriving\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  ROUND(AVG(DepDelay),1) AS AvgDepDelay,\n",
    "  ROUND(AVG(ArrDelay),1) AS AvgArrDelay\n",
    "FROM on_time_performance\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Late Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are flights late? Lets look at some delayed flights and the delay causes\n",
    "late_flights = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  ArrDelayMinutes,\n",
    "  WeatherDelay,\n",
    "  CarrierDelay,\n",
    "  NASDelay,\n",
    "  SecurityDelay,\n",
    "  LateAircraftDelay\n",
    "FROM\n",
    "  on_time_performance\n",
    "WHERE\n",
    "  WeatherDelay IS NOT NULL\n",
    "  OR\n",
    "  CarrierDelay IS NOT NULL\n",
    "  OR\n",
    "  NASDelay IS NOT NULL\n",
    "  OR\n",
    "  SecurityDelay IS NOT NULL\n",
    "  OR\n",
    "  LateAircraftDelay IS NOT NULL\n",
    "ORDER BY\n",
    "  FlightDate\n",
    "\"\"\")\n",
    "late_flights.sample(False, 0.01).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Why Flights Are Late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage contribution to delay for each source\n",
    "total_delays = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  ROUND(SUM(WeatherDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_weather_delay,\n",
    "  ROUND(SUM(CarrierDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_carrier_delay,\n",
    "  ROUND(SUM(NASDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_nas_delay,\n",
    "  ROUND(SUM(SecurityDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_security_delay,\n",
    "  ROUND(SUM(LateAircraftDelay)/SUM(ArrDelayMinutes) * 100, 1) AS pct_late_aircraft_delay\n",
    "FROM on_time_performance\n",
    "\"\"\")\n",
    "total_delays.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing a Histogram of Weather Delayed Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eyeball the first to define our buckets\n",
    "weather_delay_histogram = on_time_dataframe\\\n",
    "  .select(\"WeatherDelay\")\\\n",
    "  .rdd\\\n",
    "  .flatMap(lambda x: x)\\\n",
    "  .histogram([1, 5, 10, 15, 30, 60, 120, 240, 480, 720, 24*60.0])\n",
    "print(weather_delay_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hist(weather_delay_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a Histogram for Visualization by d3.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 15, 30, 60, 120, 240, 480, 720, 1440.0], [0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'key': 1,\n",
       " 'data': [{'label': 0, 'value': 0},\n",
       "  {'label': 15, 'value': 0},\n",
       "  {'label': 30, 'value': 0},\n",
       "  {'label': 60, 'value': 0},\n",
       "  {'label': 120, 'value': 0},\n",
       "  {'label': 240, 'value': 0},\n",
       "  {'label': 480, 'value': 0},\n",
       "  {'label': 720, 'value': 0}]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the data into something easily consumed by d3\n",
    "def histogram_to_publishable(histogram):\n",
    "  record = {'key': 1, 'data': []}\n",
    "  for label, value in zip(histogram[0], histogram[1]):\n",
    "    record['data'].append(\n",
    "      {\n",
    "        'label': label,\n",
    "        'value': value\n",
    "      }\n",
    "    )\n",
    "  return record\n",
    "\n",
    "# Recompute the weather histogram with a filter for on-time flights\n",
    "weather_delay_histogram = on_time_dataframe\\\n",
    "  .filter(\n",
    "    (on_time_dataframe.WeatherDelay != None)\n",
    "    &\n",
    "    (on_time_dataframe.WeatherDelay > 0)\n",
    "  )\\\n",
    "  .select(\"WeatherDelay\")\\\n",
    "  .rdd\\\n",
    "  .flatMap(lambda x: x)\\\n",
    "  .histogram([0, 15, 30, 60, 120, 240, 480, 720, 24*60.0])\n",
    "print(weather_delay_histogram)\n",
    "\n",
    "record = histogram_to_publishable(weather_delay_histogram)\n",
    "record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Classifier Model to Predict Flight Delays\n",
    "\n",
    "## Loading Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, DateType, TimestampType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"ArrDelay\", DoubleType(), True),     # \"ArrDelay\":5.0\n",
    "  StructField(\"CRSArrTime\", TimestampType(), True),    # \"CRSArrTime\":\"2015-12-31T03:20:00.000-08:00\"\n",
    "  StructField(\"CRSDepTime\", TimestampType(), True),    # \"CRSDepTime\":\"2015-12-31T03:05:00.000-08:00\"\n",
    "  StructField(\"Carrier\", StringType(), True),     # \"Carrier\":\"WN\"\n",
    "  StructField(\"DayOfMonth\", IntegerType(), True), # \"DayOfMonth\":31\n",
    "  StructField(\"DayOfWeek\", IntegerType(), True),  # \"DayOfWeek\":4\n",
    "  StructField(\"DayOfYear\", IntegerType(), True),  # \"DayOfYear\":365\n",
    "  StructField(\"DepDelay\", DoubleType(), True),     # \"DepDelay\":14.0\n",
    "  StructField(\"Dest\", StringType(), True),        # \"Dest\":\"SAN\"\n",
    "  StructField(\"Distance\", DoubleType(), True),     # \"Distance\":368.0\n",
    "  StructField(\"FlightDate\", DateType(), True),    # \"FlightDate\":\"2015-12-30T16:00:00.000-08:00\"\n",
    "  StructField(\"FlightNum\", StringType(), True),   # \"FlightNum\":\"6109\"\n",
    "  StructField(\"Origin\", StringType(), True),      # \"Origin\":\"TUS\"\n",
    "])\n",
    "\n",
    "features = spark.read.json(\n",
    "  \"../data/simple_flight_delay_features.jsonl.bz2\",\n",
    "  schema=schema\n",
    ")\n",
    "features.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data for Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Check for nulls in features before using Spark ML\n",
    "#\n",
    "null_counts = [(column, features.where(features[column].isNull()).count()) for column in features.columns]\n",
    "cols_with_nulls = filter(lambda x: x[1] > 0, null_counts)\n",
    "print(list(cols_with_nulls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a Route Column\n",
    "\n",
    "Demonstrating the addition of a feature to our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Add a Route variable to replace FlightNum\n",
    "#\n",
    "from pyspark.sql.functions import lit, concat\n",
    "\n",
    "features_with_route = features.withColumn(\n",
    "  'Route',\n",
    "  concat(\n",
    "    features.Origin,\n",
    "    lit('-'),\n",
    "    features.Dest\n",
    "  )\n",
    ")\n",
    "features_with_route.select(\"Origin\", \"Dest\", \"Route\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketizing ArrDelay into ArrDelayBucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Use pysmark.ml.feature.Bucketizer to bucketize ArrDelay\n",
    "#\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), -15.0, 0, 30.0, float(\"inf\")]\n",
    "bucketizer = Bucketizer(\n",
    "  splits=splits,\n",
    "  inputCol=\"ArrDelay\",\n",
    "  outputCol=\"ArrDelayBucket\"\n",
    ")\n",
    "ml_bucketized_features = bucketizer.transform(features_with_route)\n",
    "\n",
    "# Check the buckets out\n",
    "ml_bucketized_features.select(\"ArrDelay\", \"ArrDelayBucket\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Our String Fields into Numeric Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Extract features tools in with pyspark.ml.feature\n",
    "#\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "# Turn category fields into categoric feature vectors, then drop intermediate fields\n",
    "for column in [\"Carrier\", \"DayOfMonth\", \"DayOfWeek\", \"DayOfYear\",\n",
    "               \"Origin\", \"Dest\", \"Route\"]:\n",
    "  string_indexer = StringIndexer(\n",
    "    inputCol=column,\n",
    "    outputCol=column + \"_index\"\n",
    "  )\n",
    "  ml_bucketized_features = string_indexer.fit(ml_bucketized_features)\\\n",
    "                                          .transform(ml_bucketized_features)\n",
    "\n",
    "# Check out the indexes\n",
    "ml_bucketized_features.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Numeric Fields into a Single Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle continuous, numeric fields by combining them into one feature vector\n",
    "numeric_columns = [\"DepDelay\", \"Distance\"]\n",
    "index_columns = [\"Carrier_index\", \"DayOfMonth_index\",\n",
    "                   \"DayOfWeek_index\", \"DayOfYear_index\", \"Origin_index\",\n",
    "                   \"Origin_index\", \"Dest_index\", \"Route_index\"]\n",
    "vector_assembler = VectorAssembler(\n",
    "  inputCols=numeric_columns + index_columns,\n",
    "  outputCol=\"Features_vec\"\n",
    ")\n",
    "final_vectorized_features = vector_assembler.transform(ml_bucketized_features)\n",
    "\n",
    "# Drop the index columns\n",
    "for column in index_columns:\n",
    "  final_vectorized_features = final_vectorized_features.drop(column)\n",
    "\n",
    "# Check out the features\n",
    "final_vectorized_features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Our Model in an Experimental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Cross validate, train and evaluate classifier\n",
    "#\n",
    "\n",
    "# Test/train split\n",
    "training_data, test_data = final_vectorized_features.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Instantiate and fit random forest classifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(\n",
    "  featuresCol=\"Features_vec\",\n",
    "  labelCol=\"ArrDelayBucket\",\n",
    "  maxBins=4657,\n",
    "  maxMemoryInMB=1024\n",
    ")\n",
    "model = rfc.fit(training_data)\n",
    "\n",
    "# Evaluate model using test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = {}\".format(accuracy))\n",
    "\n",
    "# Check a sample\n",
    "predictions.sample(False, 0.001, 18).orderBy(\"CRSDepTime\").show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
